{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e73aa17-6166-43dc-8222-800760cbb760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os, re, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Canonical columns & Bloom sequence ----------\n",
    "COLS = [\n",
    "    \"Understandable\",\n",
    "    \"TopicRelated\",\n",
    "    \"Grammatical\",\n",
    "    \"Clear\",\n",
    "    \"Rephrase\",\n",
    "    \"Answerable\",\n",
    "    \"Central\",\n",
    "    \"WouldYouUseIt\",\n",
    "    \"Bloom’sLevel\",\n",
    "]\n",
    "BLOOM_SEQ = [\"remember\", \"understand\", \"apply\", \"analyze\", \"evaluate\", \"create\"]\n",
    "\n",
    "# ---------- Robust CSV loader (no header), preserves blank lines ----------\n",
    "def read_csv_no_header(path: str, delimiter: str = \",\", encoding: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV with NO header and force exactly 9 columns (COLS).\n",
    "    - Preserves blank lines as rows (mapped to 9 empty fields).\n",
    "    - Uses csv.reader for robust parsing; normalizes row width.\n",
    "    \"\"\"\n",
    "    rows: List[List[str]] = []\n",
    "    enc = encoding or \"utf-8-sig\"\n",
    "    with open(path, \"r\", encoding=enc, newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        for raw in reader:\n",
    "            # csv.reader returns [] for a blank line\n",
    "            if raw is None or len(raw) == 0:\n",
    "                rows.append([\"\"] * len(COLS))\n",
    "                continue\n",
    "            # Normalize to exactly 9 columns\n",
    "            if len(raw) < len(COLS):\n",
    "                raw = list(raw) + [\"\"] * (len(COLS) - len(raw))\n",
    "            elif len(raw) > len(COLS):\n",
    "                # merge extras into the last column (keeps row count aligned)\n",
    "                raw = raw[: len(COLS) - 1] + [delimiter.join(raw[len(COLS) - 1 :])]\n",
    "            rows.append(raw)\n",
    "    df = pd.DataFrame(rows, columns=COLS)\n",
    "    return df\n",
    "\n",
    "# ---------- Helpers: normalization & layout checks ----------\n",
    "def _normalize_yes_no(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip().str.lower()\n",
    "\n",
    "def infer_intended_series(n_rows: int) -> pd.Series:\n",
    "    if n_rows % 6 != 0:\n",
    "        raise ValueError(\n",
    "            \"Row count must be a multiple of 6 to infer intended Bloom levels \"\n",
    "            \"(sequence: remember→understand→apply→analyze→evaluate→create).\"\n",
    "        )\n",
    "    reps = n_rows // 6\n",
    "    values = BLOOM_SEQ * reps\n",
    "    return pd.Series(values, index=range(n_rows), name=\"IntendedBloom_Inferred\")\n",
    "\n",
    "def _row_is_all_empty(row: pd.Series) -> bool:\n",
    "    return all(str(x).strip() == \"\" for x in row.values.tolist())\n",
    "\n",
    "def enforce_expected_length(filename: str, df: pd.DataFrame, expected: int) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Try to reconcile trivial off-by-one cases by trimming leading/trailing fully empty rows.\n",
    "    Never deletes internal rows (to preserve mapping).\n",
    "    \"\"\"\n",
    "    if len(df) == expected:\n",
    "        return df, \"OK\"\n",
    "\n",
    "    status = f\"got={len(df)}, expected={expected}\"\n",
    "    # Drop trailing empty rows first\n",
    "    while len(df) > expected and _row_is_all_empty(df.iloc[-1]):\n",
    "        df = df.iloc[:-1].reset_index(drop=True)\n",
    "    # Drop leading empty rows if still too long\n",
    "    while len(df) > expected and _row_is_all_empty(df.iloc[0]):\n",
    "        df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    if len(df) == expected:\n",
    "        return df, f\"TRIMMED_EMPTY_EDGES ({status} → {len(df)})\"\n",
    "\n",
    "    # Can't fix automatically (e.g., internal extras or true missing row)\n",
    "    return df, f\"MISMATCH ({status})\"\n",
    "\n",
    "# ---------- Core metric on a DF SLICE ----------\n",
    "def compute_quality_and_skill(df: pd.DataFrame, verbose: bool = False, export_with_flag: Optional[str] = None) -> Dict[str, float]:\n",
    "    data = df.copy()\n",
    "\n",
    "    # Normalize strings used in rules\n",
    "    for c in [\"Understandable\",\"Grammatical\",\"Clear\",\"Rephrase\",\"Answerable\",\"WouldYouUseIt\",\"Bloom’sLevel\"]:\n",
    "        data[c] = _normalize_yes_no(data[c])\n",
    "\n",
    "    # High-Quality logic (paper definition)\n",
    "    cond_A = (\n",
    "        (data[\"Understandable\"] == \"yes\") &\n",
    "        (data[\"Grammatical\"] == \"yes\") &\n",
    "        (data[\"Clear\"] == \"yes\") &\n",
    "        (data[\"Answerable\"] == \"yes\") &\n",
    "        (data[\"WouldYouUseIt\"].isin([\"yes\", \"maybe\"]))\n",
    "    )\n",
    "    cond_B = (\n",
    "        (data[\"Understandable\"] == \"yes\") &\n",
    "        (data[\"Grammatical\"] == \"yes\") &\n",
    "        (data[\"Clear\"].isin([\"yes\", \"more_or_less\"])) &\n",
    "        (data[\"Rephrase\"] == \"yes\") &\n",
    "        (data[\"Answerable\"] == \"yes\")\n",
    "    )\n",
    "    data[\"HighQuality\"] = (cond_A | cond_B)\n",
    "\n",
    "    total = len(data)\n",
    "    hq_count = int(data[\"HighQuality\"].sum())\n",
    "    quality_pct = (hq_count / total * 100.0) if total else 0.0\n",
    "\n",
    "    intended = infer_intended_series(total)\n",
    "    if hq_count > 0:\n",
    "        evaluator = data.loc[data[\"HighQuality\"], \"Bloom’sLevel\"]\n",
    "        intended_aligned = intended.loc[evaluator.index]\n",
    "        skill_match_mask = (evaluator == intended_aligned)\n",
    "        skill_match_count = int(skill_match_mask.sum())\n",
    "        skill_match_pct = float(skill_match_mask.mean() * 100.0)\n",
    "    else:\n",
    "        skill_match_pct = 0.0\n",
    "        skill_match_count = 0\n",
    "\n",
    "    if export_with_flag:\n",
    "        data.to_csv(f\"{export_with_flag}.csv\", index=False)\n",
    "\n",
    "    return {\n",
    "        \"Total Questions\": total,\n",
    "        \"High-quality (#)\": hq_count,\n",
    "        \"High-quality (%)\": round(quality_pct, 2),\n",
    "        \"Skill Match (# among High-quality)\": skill_match_count,\n",
    "        \"Skill Match (% among High-quality)\": round(skill_match_pct, 2),\n",
    "    }\n",
    "\n",
    "# ---------- Aggregate per ACTUAL model across ALL topics within ONE CSV ----------\n",
    "def compute_aggregate_per_model(\n",
    "    df: pd.DataFrame,\n",
    "    model_labels_in_topic_order: List[str],\n",
    "    num_topics: int,\n",
    "    rows_per_model: int = 6,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Re-stitches non-contiguous 6-row blocks for each actual model across all topics in a single CSV.\"\"\"\n",
    "    M = len(model_labels_in_topic_order)\n",
    "    T = num_topics\n",
    "    block_span = M * rows_per_model  # rows per topic\n",
    "    expected_total = block_span * T\n",
    "\n",
    "    if len(df) != expected_total:\n",
    "        raise ValueError(\n",
    "            f\"CSV row count {len(df)} doesn't match expected {expected_total} \"\n",
    "            f\"(models per topic={M}, topics={T}, rows_per_model={rows_per_model}).\"\n",
    "        )\n",
    "\n",
    "    results: Dict[str, Dict[str, float]] = {}\n",
    "    for m_idx, label in enumerate(model_labels_in_topic_order):\n",
    "        parts = []\n",
    "        for t in range(T):\n",
    "            start = t * block_span + m_idx * rows_per_model\n",
    "            end = start + rows_per_model\n",
    "            parts.append(df.iloc[start:end])\n",
    "        gdf = pd.concat(parts, axis=0).reset_index(drop=True)\n",
    "        results[label] = compute_quality_and_skill(gdf, verbose=False)\n",
    "    return results\n",
    "\n",
    "# ---------- Public: compute tidy DF over many file tags, PER PROMPT ----------\n",
    "def run_eval_to_dataframe_per_prompt(\n",
    "    prompts: List[str],\n",
    "    file_name_tags: List[str],\n",
    "    data_model_order: List[str],\n",
    "    num_topics_per_prompt: int,\n",
    "    rows_per_model: int = 6,\n",
    "    base_dir: Optional[str] = None,\n",
    "    delimiter: str = \",\",\n",
    ") -> pd.DataFrame:\n",
    "    all_rows: List[Dict[str, object]] = []\n",
    "    M = len(data_model_order)\n",
    "    expected_rows_per_csv = M * rows_per_model * num_topics_per_prompt\n",
    "\n",
    "    for prompt in prompts:\n",
    "        for tag in file_name_tags:\n",
    "            filename = f\"{prompt}_{tag}.csv\"\n",
    "            path = os.path.join(base_dir, filename) if base_dir else filename\n",
    "\n",
    "            df = read_csv_no_header(path, delimiter=delimiter)\n",
    "            df, status = enforce_expected_length(filename, df, expected_rows_per_csv)\n",
    "            if status != \"OK\" and not status.startswith(\"TRIMMED_EMPTY_EDGES\"):\n",
    "                # Keep the error strict here to avoid mis-aggregation\n",
    "                raise ValueError(f\"[{filename}] Row count issue: {status}\")\n",
    "\n",
    "            per_model = compute_aggregate_per_model(\n",
    "                df,\n",
    "                model_labels_in_topic_order=data_model_order,\n",
    "                num_topics=num_topics_per_prompt,\n",
    "                rows_per_model=rows_per_model,\n",
    "            )\n",
    "\n",
    "            for model_label, metrics in per_model.items():\n",
    "                all_rows.append({\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"Eval-Model\": tag.rstrip(\"_clean\"),\n",
    "                    \"Model\": model_label,\n",
    "                    **metrics\n",
    "                })\n",
    "\n",
    "    cols = [\n",
    "        \"Prompt\",\n",
    "        \"Eval-Model\",\n",
    "        \"Model\",\n",
    "        \"Total Questions\",\n",
    "        \"High-quality (#)\",\n",
    "        \"High-quality (%)\",\n",
    "        \"Skill Match (# among High-quality)\",\n",
    "        \"Skill Match (% among High-quality)\",\n",
    "    ]\n",
    "    out = pd.DataFrame(all_rows)\n",
    "    if not out.empty:\n",
    "        out = out[cols].sort_values([\"Prompt\", \"Eval-Model\", \"Model\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ===================== AGREEMENT CHECK (labels workbook with 5 sheets) =====================\n",
    "\n",
    "def _to_bool(series: pd.Series) -> pd.Series:\n",
    "    truthy = {\"true\", \"yes\", \"y\", \"1\", 1, True}\n",
    "    falsy = {\"false\", \"no\", \"n\", \"0\", 0, False}\n",
    "    def cast(v):\n",
    "        if pd.isna(v):\n",
    "            return None\n",
    "        if isinstance(v, str):\n",
    "            vv = v.strip().lower()\n",
    "        else:\n",
    "            vv = v\n",
    "        if vv in truthy: return True\n",
    "        if vv in falsy: return False\n",
    "        try: return bool(int(v))\n",
    "        except Exception: return None\n",
    "    return series.map(cast)\n",
    "\n",
    "def compute_highquality_per_row(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Per-row HighQuality flag using the AEQG rules with normalization.\"\"\"\n",
    "    data = df.copy()\n",
    "    for c in [\"Understandable\",\"Grammatical\",\"Clear\",\"Rephrase\",\"Answerable\",\"WouldYouUseIt\"]:\n",
    "        data[c] = _normalize_yes_no(data[c])\n",
    "\n",
    "    cond_A = (\n",
    "        (data[\"Understandable\"] == \"yes\") &\n",
    "        (data[\"Grammatical\"] == \"yes\") &\n",
    "        (data[\"Clear\"] == \"yes\") &\n",
    "        (data[\"Answerable\"] == \"yes\") &\n",
    "        (data[\"WouldYouUseIt\"].isin([\"yes\", \"maybe\"]))\n",
    "    )\n",
    "    cond_B = (\n",
    "        (data[\"Understandable\"] == \"yes\") &\n",
    "        (data[\"Grammatical\"] == \"yes\") &\n",
    "        (data[\"Clear\"].isin([\"yes\", \"more_or_less\"])) &\n",
    "        (data[\"Rephrase\"] == \"yes\") &\n",
    "        (data[\"Answerable\"] == \"yes\")\n",
    "    )\n",
    "    return (cond_A | cond_B)\n",
    "\n",
    "def load_labels_workbook_sheet(labels_workbook_path: str, prompt: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load labels for given prompt from workbook sheet named exactly the prompt (e.g., 'PS1').\n",
    "    Columns required: Index, IsTrueHighQuality\n",
    "    \"\"\"\n",
    "    if not os.path.exists(labels_workbook_path):\n",
    "        raise FileNotFoundError(f\"Labels workbook not found: {labels_workbook_path}\")\n",
    "    df = pd.read_excel(labels_workbook_path, sheet_name=prompt)\n",
    "    needed = {\"Index\", \"IsTrueHighQuality\"}\n",
    "    missing = needed - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Sheet '{prompt}' missing columns: {missing}. Need exactly: {needed}\")\n",
    "    df = df.copy()\n",
    "    df[\"Index\"] = pd.to_numeric(df[\"Index\"], errors=\"coerce\").astype(\"Int64\")  # keep NA if any\n",
    "    df[\"IsTrueHighQuality\"] = _to_bool(df[\"IsTrueHighQuality\"])\n",
    "    return df[[\"Index\", \"IsTrueHighQuality\"]].dropna(subset=[\"Index\"]).astype({\"Index\": int}).reset_index(drop=True)\n",
    "\n",
    "# ---------- NEW: auto-detect best index shift to maximize coverage ----------\n",
    "def _best_index_shift(lab_idx: pd.Series, n_rows: int, shifts=range(-2, 3)) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Try shifts (e.g., -2..+2) and return (best_shift, matched_count) maximizing overlap with [0, n_rows-1].\n",
    "    \"\"\"\n",
    "    comp_idx_set = set(range(n_rows))\n",
    "    best = (0, -1)  # (shift, matches)\n",
    "    for s in shifts:\n",
    "        shifted = (lab_idx + s)\n",
    "        matches = int(shifted.isin(comp_idx_set).sum())\n",
    "        if matches > best[1]:\n",
    "            best = (s, matches)\n",
    "    return best\n",
    "\n",
    "def _apply_index_shift(lab_df: pd.DataFrame, shift: int, n_rows: int) -> pd.DataFrame:\n",
    "    shifted = lab_df.copy()\n",
    "    shifted[\"Index\"] = shifted[\"Index\"] + shift\n",
    "    # Keep only valid range\n",
    "    shifted = shifted[(shifted[\"Index\"] >= 0) & (shifted[\"Index\"] < n_rows)].copy()\n",
    "    # Drop duplicate indices keeping the first occurrence\n",
    "    shifted = shifted.drop_duplicates(subset=[\"Index\"], keep=\"first\").reset_index(drop=True)\n",
    "    return shifted\n",
    "\n",
    "def compute_agreement_metrics(merged: pd.DataFrame) -> dict:\n",
    "    \"\"\"Return confusion + metrics; also report how many rows had no label.\"\"\"\n",
    "    total_rows = len(merged)\n",
    "    m = merged.dropna(subset=[\"LabelIsTrueHighQuality\"]).copy()\n",
    "    unlabeled = total_rows - len(m)\n",
    "\n",
    "    tp = int(((m[\"ComputedHighQuality\"] == True)  & (m[\"LabelIsTrueHighQuality\"] == True)).sum())\n",
    "    tn = int(((m[\"ComputedHighQuality\"] == False) & (m[\"LabelIsTrueHighQuality\"] == False)).sum())\n",
    "    fp = int(((m[\"ComputedHighQuality\"] == True)  & (m[\"LabelIsTrueHighQuality\"] == False)).sum())\n",
    "    fn = int(((m[\"ComputedHighQuality\"] == False) & (m[\"LabelIsTrueHighQuality\"] == True)).sum())\n",
    "    total_labeled = len(m)\n",
    "\n",
    "    acc = (tp + tn) / total_labeled * 100.0 if total_labeled else 0.0\n",
    "    prec = tp / (tp + fp) * 100.0 if (tp + fp) else 0.0\n",
    "    rec = tp / (tp + fn) * 100.0 if (tp + fn) else 0.0\n",
    "    f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"Total Rows\": int(total_rows),\n",
    "        \"Total Labeled\": int(total_labeled),\n",
    "        \"Unlabeled (#)\": int(unlabeled),\n",
    "        \"Accuracy (%)\": round(acc, 2),\n",
    "        \"Precision (%)\": round(prec, 2),\n",
    "        \"Recall (%)\": round(rec, 2),\n",
    "        \"F1 (%)\": round(f1, 2),\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn,\n",
    "    }\n",
    "\n",
    "# ---------- Assign per-row ACTUAL model label based on layout ----------\n",
    "def assign_model_labels(n_rows: int, data_model_order: List[str], rows_per_model: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Assign the actual model for each row given the per-topic layout:\n",
    "    inside each topic, rows are in DATA_MODEL_ORDER and each model contributes rows_per_model rows.\n",
    "    \"\"\"\n",
    "    M = len(data_model_order)\n",
    "    block_span = M * rows_per_model\n",
    "    if block_span == 0:\n",
    "        raise ValueError(\"rows_per_model or number of models is zero.\")\n",
    "    model_idx = [((i % block_span) // rows_per_model) for i in range(n_rows)]\n",
    "    labels = [data_model_order[j] for j in model_idx]\n",
    "    return pd.Series(labels, index=range(n_rows), name=\"Model\")\n",
    "\n",
    "# ---------- Excel sheet-name sanitizer ----------\n",
    "def make_safe_sheet_name(name: str, used: set) -> str:\n",
    "    base = os.path.splitext(name)[0]\n",
    "    safe = re.sub(r\"[\\[\\]\\:\\*\\?\\/\\\\]\", \"·\", base).strip().strip(\"'\")\n",
    "    MAXLEN = 31\n",
    "    safe = safe[:MAXLEN] or \"Sheet\"\n",
    "    candidate = safe\n",
    "    i = 1\n",
    "    while candidate in used:\n",
    "        suffix = f\"~{i}\"\n",
    "        candidate = (safe[:MAXLEN - len(suffix)]) + suffix\n",
    "        i += 1\n",
    "    used.add(candidate)\n",
    "    return candidate\n",
    "\n",
    "# ---------- Safe recomputation of metrics from sums (no warnings) ----------\n",
    "def _recompute_metrics_from_sums(df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a df with columns TP, FP, FN, TN, Total Rows, Total Labeled, Unlabeled (#),\n",
    "    compute Accuracy/Precision/Recall/F1 (masked division).\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Ensure numeric dtype\n",
    "    for c in [\"TP\",\"FP\",\"FN\",\"TN\",\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\"]:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    tp = out[\"TP\"].to_numpy(dtype=float)\n",
    "    fp = out[\"FP\"].to_numpy(dtype=float)\n",
    "    fn = out[\"FN\"].to_numpy(dtype=float)\n",
    "    tn = out[\"TN\"].to_numpy(dtype=float)\n",
    "    tot_lab = out[\"Total Labeled\"].to_numpy(dtype=float)\n",
    "\n",
    "    def safe_div(num: np.ndarray, den: np.ndarray) -> np.ndarray:\n",
    "        res = np.zeros_like(num, dtype=float)\n",
    "        np.divide(num, den, out=res, where=(den > 0))\n",
    "        return res\n",
    "\n",
    "    acc_r  = safe_div(tp + tn, tot_lab)\n",
    "    prec_r = safe_div(tp, tp + fp)\n",
    "    rec_r  = safe_div(tp, tp + fn)\n",
    "    f1_r   = safe_div(2.0 * prec_r * rec_r, (prec_r + rec_r))\n",
    "\n",
    "    out[\"Accuracy (%)\"]  = np.round(acc_r * 100.0, 2)\n",
    "    out[\"Precision (%)\"] = np.round(prec_r * 100.0, 2)\n",
    "    out[\"Recall (%)\"]    = np.round(rec_r * 100.0, 2)\n",
    "    out[\"F1 (%)\"]        = np.round(f1_r * 100.0, 2)\n",
    "\n",
    "    ordered = group_cols + [\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\",\n",
    "                            \"Accuracy (%)\",\"Precision (%)\",\"Recall (%)\",\"F1 (%)\"]\n",
    "    return out[ordered]\n",
    "\n",
    "def run_quality_agreement_check_with_workbook(\n",
    "    prompts: List[str],\n",
    "    file_name_tags: List[str],\n",
    "    base_dir: Optional[str],\n",
    "    delimiter: str,\n",
    "    labels_workbook_path: str,\n",
    "    data_model_order: List[str],\n",
    "    rows_per_model: int,\n",
    "    num_topics_per_prompt: int,\n",
    "    export_excel_path: Optional[str] = \"AEQG_quality_agreement.xlsx\",\n",
    ") -> Tuple[pd.DataFrame, dict, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each prompt P in prompts:\n",
    "      - Load labels from workbook sheet named P (Index, IsTrueHighQuality).\n",
    "      - For each file <P>_<tag>.csv:\n",
    "          * Robustly read rows (preserve blanks), enforce expected rows = M*rows_per_model*num_topics\n",
    "          * Compute ComputedHighQuality per row\n",
    "          * Attach per-row ACTUAL Model via layout\n",
    "          * Auto-detect label Index shift (−2..+2) to maximize coverage\n",
    "          * Merge labels on Index\n",
    "      - Produce:\n",
    "          1) summary_file_df (per file)\n",
    "          2) agree_by_eval_model_df (Prompt × Eval-Model)\n",
    "          3) agree_by_model_df (Prompt × Eval-Model × Model)\n",
    "      - detail_per_file: raw merged rows per file\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "    detail_per_file: Dict[str, pd.DataFrame] = {}\n",
    "    by_eval_rows = []\n",
    "    by_model_rows = []\n",
    "\n",
    "    M = len(data_model_order)\n",
    "    expected_rows_per_csv = M * rows_per_model * num_topics_per_prompt\n",
    "\n",
    "    # Cache labels per prompt so we read each sheet once\n",
    "    labels_cache: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    for prompt in prompts:\n",
    "        if prompt not in labels_cache:\n",
    "            labels_cache[prompt] = load_labels_workbook_sheet(labels_workbook_path, prompt)\n",
    "        lab_df_base = labels_cache[prompt]\n",
    "\n",
    "        for tag in file_name_tags:\n",
    "            filename = f\"{prompt}_{tag}.csv\"\n",
    "            path = os.path.join(base_dir, filename) if base_dir else filename\n",
    "\n",
    "            df = read_csv_no_header(path, delimiter=delimiter)\n",
    "            df, status = enforce_expected_length(filename, df, expected_rows_per_csv)\n",
    "            if status != \"OK\" and not status.startswith(\"TRIMMED_EMPTY_EDGES\"):\n",
    "                print(f\"[WARN] {filename}: {status} — proceeding with agreement (mapping still index-based).\")\n",
    "\n",
    "            n = len(df)\n",
    "\n",
    "            # Per-row flags & model label\n",
    "            comp_hq = compute_highquality_per_row(df).astype(bool)\n",
    "            model_series = assign_model_labels(n, data_model_order=data_model_order, rows_per_model=rows_per_model)\n",
    "            comp_df = pd.DataFrame({\n",
    "                \"Index\": range(n),\n",
    "                \"ComputedHighQuality\": comp_hq.values,\n",
    "                \"Model\": model_series.values,\n",
    "                \"Prompt\": prompt,\n",
    "                \"Eval-Model\": tag.rstrip(\"_clean\"),\n",
    "                \"File\": filename,\n",
    "            })\n",
    "\n",
    "            # ----- Auto-detect best index shift -----\n",
    "            shift, matched = _best_index_shift(lab_df_base[\"Index\"], n_rows=n, shifts=range(-2, 3))\n",
    "            lab_df_prompt = _apply_index_shift(lab_df_base, shift=shift, n_rows=n)\n",
    "\n",
    "            merged = comp_df.merge(\n",
    "                lab_df_prompt.rename(columns={\"IsTrueHighQuality\": \"LabelIsTrueHighQuality\"}),\n",
    "                on=\"Index\",\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            # ---------- (1) Per-file summary ----------\n",
    "            size_note = f\"labels={len(lab_df_prompt)}/orig={len(lab_df_base)}, data={n}, shift={shift}, matched={matched}\"\n",
    "            metrics_file = compute_agreement_metrics(merged)\n",
    "            summary_rows.append({\n",
    "                \"Prompt\": prompt,\n",
    "                \"Eval-Model\": tag.rstrip(\"_clean\"),\n",
    "                \"File\": filename,\n",
    "                \"RowCountCheck\": f\"{status}; {size_note}\" if status != \"OK\" else size_note,\n",
    "                **metrics_file\n",
    "            })\n",
    "\n",
    "            # ---------- (2) By Eval-Model (for this prompt/tag): store counts; aggregate later ----------\n",
    "            m_eval = compute_agreement_metrics(merged)\n",
    "            by_eval_rows.append({\n",
    "                \"Prompt\": prompt,\n",
    "                \"Eval-Model\": tag.rstrip(\"_clean\"),\n",
    "                **{k: m_eval[k] for k in [\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\"]}\n",
    "            })\n",
    "\n",
    "            # ---------- (3) By Model within this file ----------\n",
    "            for model_label, g in merged.groupby(\"Model\", sort=False):\n",
    "                mcounts = compute_agreement_metrics(g)\n",
    "                by_model_rows.append({\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"Eval-Model\": tag.rstrip(\"_clean\"),\n",
    "                    \"Model\": model_label,\n",
    "                    **{k: mcounts[k] for k in [\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\"]}\n",
    "                })\n",
    "\n",
    "            merged[\"Match\"] = (merged[\"ComputedHighQuality\"] == merged[\"LabelIsTrueHighQuality\"])\n",
    "            detail_per_file[filename] = merged\n",
    "\n",
    "    # Build DataFrames\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values([\"Prompt\", \"Eval-Model\", \"File\"]).reset_index(drop=True)\n",
    "\n",
    "    # Aggregate Eval-Model rows (sums), then recompute metrics from sums (masked division)\n",
    "    if len(by_eval_rows):\n",
    "        tmp = pd.DataFrame(by_eval_rows)\n",
    "        grouped = tmp.groupby([\"Prompt\",\"Eval-Model\"], as_index=False)[\n",
    "            [\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\"]\n",
    "        ].sum()\n",
    "        agree_by_eval_model_df = _recompute_metrics_from_sums(grouped, [\"Prompt\",\"Eval-Model\"])\n",
    "    else:\n",
    "        agree_by_eval_model_df = pd.DataFrame(columns=[\n",
    "            \"Prompt\",\"Eval-Model\",\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\",\n",
    "            \"Accuracy (%)\",\"Precision (%)\",\"Recall (%)\",\"F1 (%)\"\n",
    "        ])\n",
    "\n",
    "    # Aggregate by model (Prompt × Eval-Model × Model)\n",
    "    if len(by_model_rows):\n",
    "        tmpm = pd.DataFrame(by_model_rows)\n",
    "        groupedm = tmpm.groupby([\"Prompt\",\"Eval-Model\",\"Model\"], as_index=False)[\n",
    "            [\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\"]\n",
    "        ].sum()\n",
    "        agree_by_model_df = _recompute_metrics_from_sums(groupedm, [\"Prompt\",\"Eval-Model\",\"Model\"]).sort_values(\n",
    "            [\"Prompt\",\"Eval-Model\",\"Model\"]\n",
    "        ).reset_index(drop=True)\n",
    "    else:\n",
    "        agree_by_model_df = pd.DataFrame(columns=[\n",
    "            \"Prompt\",\"Eval-Model\",\"Model\",\"Total Rows\",\"Total Labeled\",\"Unlabeled (#)\",\"TP\",\"FP\",\"FN\",\"TN\",\n",
    "            \"Accuracy (%)\",\"Precision (%)\",\"Recall (%)\",\"F1 (%)\"\n",
    "        ])\n",
    "\n",
    "    # Export\n",
    "    if export_excel_path:\n",
    "        with pd.ExcelWriter(export_excel_path, engine=\"xlsxwriter\") as writer:\n",
    "            summary_df.to_excel(writer, index=False, sheet_name=\"SUMMARY\")\n",
    "            agree_by_eval_model_df.to_excel(writer, index=False, sheet_name=\"AGREE_BY_EVAL_MODEL\")\n",
    "            agree_by_model_df.to_excel(writer, index=False, sheet_name=\"AGREE_BY_MODEL\")\n",
    "\n",
    "            used_names: set = {\"SUMMARY\", \"AGREE_BY_EVAL_MODEL\", \"AGREE_BY_MODEL\"}\n",
    "            for fname, detail_df in detail_per_file.items():\n",
    "                sheet = make_safe_sheet_name(fname, used_names)\n",
    "                detail_df.to_excel(writer, index=False, sheet_name=sheet)\n",
    "\n",
    "    # Also quick CSVs\n",
    "    agree_by_eval_model_df.to_csv(\"AEQG_agreement_by_eval_model.csv\", index=False)\n",
    "    agree_by_model_df.to_csv(\"AEQG_agreement_by_model.csv\", index=False)\n",
    "\n",
    "    return summary_df, detail_per_file, agree_by_eval_model_df, agree_by_model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f38152f-35fb-4fac-bb5c-2fede6e7ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== DEFAULTS / EXAMPLE MAIN =====================\n",
    "\n",
    "PROMPTS = ['PS1', 'PS2', 'PS3', 'PS4', 'PS5']\n",
    "FILE_TAGS = [\n",
    "    'deepseek-r1:14b',\n",
    "    'phi4:latest',\n",
    "    'gemma3:latest',\n",
    "    'mistral-small3.2:latest',\n",
    "    'phi4-mini:latest',\n",
    "    'granite4:latest',\n",
    "    'llama3.2:latest',\n",
    "    'gpt-oss:latest',\n",
    "]\n",
    "FILE_TAGS = [f\"{item}_clean\" for item in FILE_TAGS]\n",
    "\n",
    "DATA_MODEL_ORDER = [\"GPT4\", \"GPT3.5\", \"Palm2\", \"Llama2_70B\", \"Mistral_7B\"]\n",
    "NUM_TOPICS_PER_PROMPT = 17\n",
    "ROWS_PER_MODEL = 6  # Bloom cycle length\n",
    "\n",
    "BASE_DIR = \"clean_output_full\"\n",
    "DELIM = \",\"\n",
    "LABELS_WORKBOOK = \"All_PS_ExpertEvaluation_withExists.xlsx\"  # sheets: PS1..PS5 with Index, IsTrueHighQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2eddcf-2878-4979-8d75-3269fc747385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AEQG Metrics (Per Prompt × Eval-Model × Model) ===\n",
      "    Prompt       Eval-Model       Model  Total Questions  High-quality (#)  \\\n",
      "0      PS1  deepseek-r1:14b      GPT3.5              102                95   \n",
      "1      PS1  deepseek-r1:14b        GPT4              102                93   \n",
      "2      PS1  deepseek-r1:14b  Llama2_70B              102                95   \n",
      "3      PS1  deepseek-r1:14b  Mistral_7B              102                90   \n",
      "4      PS1  deepseek-r1:14b       Palm2              102                94   \n",
      "..     ...              ...         ...              ...               ...   \n",
      "195    PS5      phi4:latest      GPT3.5              102                62   \n",
      "196    PS5      phi4:latest        GPT4              102                82   \n",
      "197    PS5      phi4:latest  Llama2_70B              102                44   \n",
      "198    PS5      phi4:latest  Mistral_7B              102                47   \n",
      "199    PS5      phi4:latest       Palm2              102                58   \n",
      "\n",
      "     High-quality (%)  Skill Match (# among High-quality)  \\\n",
      "0               93.14                                  77   \n",
      "1               91.18                                  64   \n",
      "2               93.14                                  65   \n",
      "3               88.24                                  47   \n",
      "4               92.16                                  63   \n",
      "..                ...                                 ...   \n",
      "195             60.78                                  27   \n",
      "196             80.39                                  28   \n",
      "197             43.14                                  21   \n",
      "198             46.08                                  20   \n",
      "199             56.86                                  18   \n",
      "\n",
      "     Skill Match (% among High-quality)  \n",
      "0                                 81.05  \n",
      "1                                 68.82  \n",
      "2                                 68.42  \n",
      "3                                 52.22  \n",
      "4                                 67.02  \n",
      "..                                  ...  \n",
      "195                               43.55  \n",
      "196                               34.15  \n",
      "197                               47.73  \n",
      "198                               42.55  \n",
      "199                               31.03  \n",
      "\n",
      "[200 rows x 8 columns]\n",
      "\n",
      "=== Agreement Summary (Per File) ===\n",
      "   Prompt               Eval-Model                                   File  \\\n",
      "0     PS1          deepseek-r1:14b          PS1_deepseek-r1:14b_clean.csv   \n",
      "1     PS1            gemma3:latest            PS1_gemma3:latest_clean.csv   \n",
      "2     PS1           gpt-oss:latest           PS1_gpt-oss:latest_clean.csv   \n",
      "3     PS1          granite4:latest          PS1_granite4:latest_clean.csv   \n",
      "4     PS1          llama3.2:latest          PS1_llama3.2:latest_clean.csv   \n",
      "5     PS1  mistral-small3.2:latest  PS1_mistral-small3.2:latest_clean.csv   \n",
      "6     PS1         phi4-mini:latest         PS1_phi4-mini:latest_clean.csv   \n",
      "7     PS1              phi4:latest              PS1_phi4:latest_clean.csv   \n",
      "8     PS2          deepseek-r1:14b          PS2_deepseek-r1:14b_clean.csv   \n",
      "9     PS2            gemma3:latest            PS2_gemma3:latest_clean.csv   \n",
      "10    PS2           gpt-oss:latest           PS2_gpt-oss:latest_clean.csv   \n",
      "11    PS2          granite4:latest          PS2_granite4:latest_clean.csv   \n",
      "12    PS2          llama3.2:latest          PS2_llama3.2:latest_clean.csv   \n",
      "13    PS2  mistral-small3.2:latest  PS2_mistral-small3.2:latest_clean.csv   \n",
      "14    PS2         phi4-mini:latest         PS2_phi4-mini:latest_clean.csv   \n",
      "15    PS2              phi4:latest              PS2_phi4:latest_clean.csv   \n",
      "16    PS3          deepseek-r1:14b          PS3_deepseek-r1:14b_clean.csv   \n",
      "17    PS3            gemma3:latest            PS3_gemma3:latest_clean.csv   \n",
      "18    PS3           gpt-oss:latest           PS3_gpt-oss:latest_clean.csv   \n",
      "19    PS3          granite4:latest          PS3_granite4:latest_clean.csv   \n",
      "20    PS3          llama3.2:latest          PS3_llama3.2:latest_clean.csv   \n",
      "21    PS3  mistral-small3.2:latest  PS3_mistral-small3.2:latest_clean.csv   \n",
      "22    PS3         phi4-mini:latest         PS3_phi4-mini:latest_clean.csv   \n",
      "23    PS3              phi4:latest              PS3_phi4:latest_clean.csv   \n",
      "24    PS4          deepseek-r1:14b          PS4_deepseek-r1:14b_clean.csv   \n",
      "25    PS4            gemma3:latest            PS4_gemma3:latest_clean.csv   \n",
      "26    PS4           gpt-oss:latest           PS4_gpt-oss:latest_clean.csv   \n",
      "27    PS4          granite4:latest          PS4_granite4:latest_clean.csv   \n",
      "28    PS4          llama3.2:latest          PS4_llama3.2:latest_clean.csv   \n",
      "29    PS4  mistral-small3.2:latest  PS4_mistral-small3.2:latest_clean.csv   \n",
      "30    PS4         phi4-mini:latest         PS4_phi4-mini:latest_clean.csv   \n",
      "31    PS4              phi4:latest              PS4_phi4:latest_clean.csv   \n",
      "32    PS5          deepseek-r1:14b          PS5_deepseek-r1:14b_clean.csv   \n",
      "33    PS5            gemma3:latest            PS5_gemma3:latest_clean.csv   \n",
      "34    PS5           gpt-oss:latest           PS5_gpt-oss:latest_clean.csv   \n",
      "35    PS5          granite4:latest          PS5_granite4:latest_clean.csv   \n",
      "36    PS5          llama3.2:latest          PS5_llama3.2:latest_clean.csv   \n",
      "37    PS5  mistral-small3.2:latest  PS5_mistral-small3.2:latest_clean.csv   \n",
      "38    PS5         phi4-mini:latest         PS5_phi4-mini:latest_clean.csv   \n",
      "39    PS5              phi4:latest              PS5_phi4:latest_clean.csv   \n",
      "\n",
      "                                        RowCountCheck  Total Rows  \\\n",
      "0   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "1   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "2   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "3   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "4   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "5   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "6   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "7   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "8   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "9   labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "10  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "11  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "12  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "13  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "14  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "15  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "16  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "17  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "18  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "19  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "20  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "21  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "22  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "23  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "24  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "25  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "26  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "27  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "28  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "29  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "30  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "31  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "32  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "33  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "34  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "35  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "36  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "37  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "38  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "39  labels=510/orig=510, data=510, shift=-1, match...         510   \n",
      "\n",
      "    Total Labeled  Unlabeled (#)  Accuracy (%)  Precision (%)  Recall (%)  \\\n",
      "0             510              0         46.08          42.40       97.06   \n",
      "1             510              0         52.55          29.35       13.24   \n",
      "2             510              0         47.84          42.92       92.16   \n",
      "3             510              0         58.63          47.80       37.25   \n",
      "4             510              0         50.78          44.08       85.78   \n",
      "5             510              0         44.12          41.54       97.55   \n",
      "6             510              0         60.78          75.00        2.94   \n",
      "7             510              0         58.04          48.38       73.04   \n",
      "8             510              0         78.43          80.82       94.70   \n",
      "9             510              0         29.41          75.00       13.64   \n",
      "10            510              0         75.49          84.65       83.59   \n",
      "11            510              0         39.41          85.37       26.52   \n",
      "12            510              0         67.84          82.04       75.00   \n",
      "13            510              0         79.02          80.04       97.22   \n",
      "14            510              0         22.75          75.00        0.76   \n",
      "15            510              0         60.39          87.89       56.82   \n",
      "16            510              0         74.71          76.09       96.32   \n",
      "17            510              0         31.18          65.26       16.32   \n",
      "18            510              0         67.45          78.16       78.16   \n",
      "19            510              0         42.16          86.32       26.58   \n",
      "20            510              0         63.92          80.06       68.68   \n",
      "21            510              0         81.18          82.42       95.00   \n",
      "22            510              0         25.69         100.00        0.26   \n",
      "23            510              0         68.63          89.29       65.79   \n",
      "24            510              0         74.90          75.37       97.30   \n",
      "25            510              0         32.94          66.29       15.90   \n",
      "26            510              0         66.27          76.12       78.17   \n",
      "27            510              0         40.00          75.19       26.15   \n",
      "28            510              0         61.76          76.67       68.19   \n",
      "29            510              0         76.47          77.58       95.15   \n",
      "30            510              0         27.25           0.00        0.00   \n",
      "31            510              0         66.08          84.86       64.96   \n",
      "32            510              0         74.51          75.27       95.59   \n",
      "33            510              0         33.53          65.00       14.33   \n",
      "34            510              0         64.51          75.71       73.83   \n",
      "35            510              0         44.90          84.17       27.82   \n",
      "36            510              0         69.41          80.71       74.93   \n",
      "37            510              0         80.98          81.52       94.77   \n",
      "38            510              0         28.82           0.00        0.00   \n",
      "39            510              0         72.55          88.05       71.07   \n",
      "\n",
      "    F1 (%)   TP   FP   FN   TN  \n",
      "0    59.02  198  269    6   37  \n",
      "1    18.24   27   65  177  241  \n",
      "2    58.57  188  250   16   56  \n",
      "3    41.87   76   83  128  223  \n",
      "4    58.24  175  222   29   84  \n",
      "5    58.27  199  280    5   26  \n",
      "6     5.66    6    2  198  304  \n",
      "7    58.20  149  159   55  147  \n",
      "8    87.21  375   89   21   25  \n",
      "9    23.08   54   18  342   96  \n",
      "10   84.12  331   60   65   54  \n",
      "11   40.46  105   18  291   96  \n",
      "12   78.36  297   65   99   49  \n",
      "13   87.80  385   96   11   18  \n",
      "14    1.50    3    1  393  113  \n",
      "15   69.02  225   31  171   83  \n",
      "16   85.02  366  115   14   15  \n",
      "17   26.11   62   33  318   97  \n",
      "18   78.16  297   83   83   47  \n",
      "19   40.64  101   16  279  114  \n",
      "20   73.94  261   65  119   65  \n",
      "21   88.26  361   77   19   53  \n",
      "22    0.52    1    0  379  130  \n",
      "23   75.76  250   30  130  100  \n",
      "24   84.94  361  118   10   21  \n",
      "25   25.65   59   30  312  109  \n",
      "26   77.13  290   91   81   48  \n",
      "27   38.80   97   32  274  107  \n",
      "28   72.18  253   77  118   62  \n",
      "29   85.47  353  102   18   37  \n",
      "30    0.00    0    0  371  139  \n",
      "31   73.59  241   43  130   96  \n",
      "32   84.22  347  114   16   33  \n",
      "33   23.48   52   28  311  119  \n",
      "34   74.76  268   86   95   61  \n",
      "35   41.82  101   19  262  128  \n",
      "36   77.71  272   65   91   82  \n",
      "37   87.64  344   78   19   69  \n",
      "38    0.00    0    0  363  147  \n",
      "39   78.66  258   35  105  112  \n",
      "\n",
      "=== Agreement by Eval-Model ===\n",
      "   Prompt               Eval-Model  Total Rows  Total Labeled  Unlabeled (#)  \\\n",
      "0     PS1          deepseek-r1:14b         510            510              0   \n",
      "1     PS1            gemma3:latest         510            510              0   \n",
      "2     PS1           gpt-oss:latest         510            510              0   \n",
      "3     PS1          granite4:latest         510            510              0   \n",
      "4     PS1          llama3.2:latest         510            510              0   \n",
      "5     PS1  mistral-small3.2:latest         510            510              0   \n",
      "6     PS1         phi4-mini:latest         510            510              0   \n",
      "7     PS1              phi4:latest         510            510              0   \n",
      "8     PS2          deepseek-r1:14b         510            510              0   \n",
      "9     PS2            gemma3:latest         510            510              0   \n",
      "10    PS2           gpt-oss:latest         510            510              0   \n",
      "11    PS2          granite4:latest         510            510              0   \n",
      "12    PS2          llama3.2:latest         510            510              0   \n",
      "13    PS2  mistral-small3.2:latest         510            510              0   \n",
      "14    PS2         phi4-mini:latest         510            510              0   \n",
      "15    PS2              phi4:latest         510            510              0   \n",
      "16    PS3          deepseek-r1:14b         510            510              0   \n",
      "17    PS3            gemma3:latest         510            510              0   \n",
      "18    PS3           gpt-oss:latest         510            510              0   \n",
      "19    PS3          granite4:latest         510            510              0   \n",
      "20    PS3          llama3.2:latest         510            510              0   \n",
      "21    PS3  mistral-small3.2:latest         510            510              0   \n",
      "22    PS3         phi4-mini:latest         510            510              0   \n",
      "23    PS3              phi4:latest         510            510              0   \n",
      "24    PS4          deepseek-r1:14b         510            510              0   \n",
      "25    PS4            gemma3:latest         510            510              0   \n",
      "26    PS4           gpt-oss:latest         510            510              0   \n",
      "27    PS4          granite4:latest         510            510              0   \n",
      "28    PS4          llama3.2:latest         510            510              0   \n",
      "29    PS4  mistral-small3.2:latest         510            510              0   \n",
      "30    PS4         phi4-mini:latest         510            510              0   \n",
      "31    PS4              phi4:latest         510            510              0   \n",
      "32    PS5          deepseek-r1:14b         510            510              0   \n",
      "33    PS5            gemma3:latest         510            510              0   \n",
      "34    PS5           gpt-oss:latest         510            510              0   \n",
      "35    PS5          granite4:latest         510            510              0   \n",
      "36    PS5          llama3.2:latest         510            510              0   \n",
      "37    PS5  mistral-small3.2:latest         510            510              0   \n",
      "38    PS5         phi4-mini:latest         510            510              0   \n",
      "39    PS5              phi4:latest         510            510              0   \n",
      "\n",
      "     TP   FP   FN   TN  Accuracy (%)  Precision (%)  Recall (%)  F1 (%)  \n",
      "0   198  269    6   37         46.08          42.40       97.06   59.02  \n",
      "1    27   65  177  241         52.55          29.35       13.24   18.24  \n",
      "2   188  250   16   56         47.84          42.92       92.16   58.57  \n",
      "3    76   83  128  223         58.63          47.80       37.25   41.87  \n",
      "4   175  222   29   84         50.78          44.08       85.78   58.24  \n",
      "5   199  280    5   26         44.12          41.54       97.55   58.27  \n",
      "6     6    2  198  304         60.78          75.00        2.94    5.66  \n",
      "7   149  159   55  147         58.04          48.38       73.04   58.20  \n",
      "8   375   89   21   25         78.43          80.82       94.70   87.21  \n",
      "9    54   18  342   96         29.41          75.00       13.64   23.08  \n",
      "10  331   60   65   54         75.49          84.65       83.59   84.12  \n",
      "11  105   18  291   96         39.41          85.37       26.52   40.46  \n",
      "12  297   65   99   49         67.84          82.04       75.00   78.36  \n",
      "13  385   96   11   18         79.02          80.04       97.22   87.80  \n",
      "14    3    1  393  113         22.75          75.00        0.76    1.50  \n",
      "15  225   31  171   83         60.39          87.89       56.82   69.02  \n",
      "16  366  115   14   15         74.71          76.09       96.32   85.02  \n",
      "17   62   33  318   97         31.18          65.26       16.32   26.11  \n",
      "18  297   83   83   47         67.45          78.16       78.16   78.16  \n",
      "19  101   16  279  114         42.16          86.32       26.58   40.64  \n",
      "20  261   65  119   65         63.92          80.06       68.68   73.94  \n",
      "21  361   77   19   53         81.18          82.42       95.00   88.26  \n",
      "22    1    0  379  130         25.69         100.00        0.26    0.52  \n",
      "23  250   30  130  100         68.63          89.29       65.79   75.76  \n",
      "24  361  118   10   21         74.90          75.37       97.30   84.94  \n",
      "25   59   30  312  109         32.94          66.29       15.90   25.65  \n",
      "26  290   91   81   48         66.27          76.12       78.17   77.13  \n",
      "27   97   32  274  107         40.00          75.19       26.15   38.80  \n",
      "28  253   77  118   62         61.76          76.67       68.19   72.18  \n",
      "29  353  102   18   37         76.47          77.58       95.15   85.47  \n",
      "30    0    0  371  139         27.25           0.00        0.00    0.00  \n",
      "31  241   43  130   96         66.08          84.86       64.96   73.59  \n",
      "32  347  114   16   33         74.51          75.27       95.59   84.22  \n",
      "33   52   28  311  119         33.53          65.00       14.33   23.48  \n",
      "34  268   86   95   61         64.51          75.71       73.83   74.76  \n",
      "35  101   19  262  128         44.90          84.17       27.82   41.82  \n",
      "36  272   65   91   82         69.41          80.71       74.93   77.71  \n",
      "37  344   78   19   69         80.98          81.52       94.77   87.64  \n",
      "38    0    0  363  147         28.82           0.00        0.00    0.00  \n",
      "39  258   35  105  112         72.55          88.05       71.07   78.66  \n",
      "\n",
      "=== Agreement by Model (Prompt × Eval-Model × Model) ===\n",
      "    Prompt       Eval-Model       Model  Total Rows  Total Labeled  \\\n",
      "0      PS1  deepseek-r1:14b      GPT3.5         102            102   \n",
      "1      PS1  deepseek-r1:14b        GPT4         102            102   \n",
      "2      PS1  deepseek-r1:14b  Llama2_70B         102            102   \n",
      "3      PS1  deepseek-r1:14b  Mistral_7B         102            102   \n",
      "4      PS1  deepseek-r1:14b       Palm2         102            102   \n",
      "..     ...              ...         ...         ...            ...   \n",
      "195    PS5      phi4:latest      GPT3.5         102            102   \n",
      "196    PS5      phi4:latest        GPT4         102            102   \n",
      "197    PS5      phi4:latest  Llama2_70B         102            102   \n",
      "198    PS5      phi4:latest  Mistral_7B         102            102   \n",
      "199    PS5      phi4:latest       Palm2         102            102   \n",
      "\n",
      "     Unlabeled (#)  TP  FP  FN  TN  Accuracy (%)  Precision (%)  Recall (%)  \\\n",
      "0                0  66  29   3   4         68.63          69.47       95.65   \n",
      "1                0  70  23   2   7         75.49          75.27       97.22   \n",
      "2                0   0  95   0   7          6.86           0.00        0.00   \n",
      "3                0   0  90   0  12         11.76           0.00        0.00   \n",
      "4                0  62  32   1   7         67.65          65.96       98.41   \n",
      "..             ...  ..  ..  ..  ..           ...            ...         ...   \n",
      "195              0  58   4  35   5         61.76          93.55       62.37   \n",
      "196              0  80   2  17   3         81.37          97.56       82.47   \n",
      "197              0  32  12  11  47         77.45          72.73       74.42   \n",
      "198              0  40   7  21  34         72.55          85.11       65.57   \n",
      "199              0  48  10  21  23         69.61          82.76       69.57   \n",
      "\n",
      "     F1 (%)  \n",
      "0     80.49  \n",
      "1     84.85  \n",
      "2      0.00  \n",
      "3      0.00  \n",
      "4     78.98  \n",
      "..      ...  \n",
      "195   74.84  \n",
      "196   89.39  \n",
      "197   73.56  \n",
      "198   74.07  \n",
      "199   75.59  \n",
      "\n",
      "[200 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Original per-model metrics (unchanged)\n",
    "df_metrics = run_eval_to_dataframe_per_prompt(\n",
    "    prompts=PROMPTS,\n",
    "    file_name_tags=FILE_TAGS,\n",
    "    data_model_order=DATA_MODEL_ORDER,\n",
    "    num_topics_per_prompt=NUM_TOPICS_PER_PROMPT,\n",
    "    rows_per_model=ROWS_PER_MODEL,\n",
    "    base_dir=BASE_DIR,\n",
    "    delimiter=DELIM,\n",
    ")\n",
    "print(\"\\n=== AEQG Metrics (Per Prompt × Eval-Model × Model) ===\")\n",
    "print(df_metrics)\n",
    "df_metrics.to_excel('AEQG_temp_evals.xlsx', index=False)\n",
    "\n",
    "# 2) Agreement with Eval-Model–wise and Model–wise splits + auto index shift + unlabeled reporting\n",
    "summary_df, detail, agree_by_eval_model_df, agree_by_model_df = run_quality_agreement_check_with_workbook(\n",
    "    prompts=PROMPTS,\n",
    "    file_name_tags=FILE_TAGS,\n",
    "    base_dir=BASE_DIR,\n",
    "    delimiter=DELIM,\n",
    "    labels_workbook_path=LABELS_WORKBOOK,\n",
    "    data_model_order=DATA_MODEL_ORDER,\n",
    "    rows_per_model=ROWS_PER_MODEL,\n",
    "    num_topics_per_prompt=NUM_TOPICS_PER_PROMPT,\n",
    "    export_excel_path=\"AEQG_quality_agreement.xlsx\",\n",
    ")\n",
    "print(\"\\n=== Agreement Summary (Per File) ===\")\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\n=== Agreement by Eval-Model ===\")\n",
    "print(agree_by_eval_model_df)\n",
    "\n",
    "print(\"\\n=== Agreement by Model (Prompt × Eval-Model × Model) ===\")\n",
    "print(agree_by_model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c98cee-65c8-4aa2-9dd2-efd16ad080f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb33c14-b0a9-45cc-aea0-a0788ac61ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
